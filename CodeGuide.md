# RootFinderGPU â€“Â Code Guide

---

## 1Â Purpose

RootFinderGPU is a minimal C# **rootâ€‘finding library** that implements the classic **Newtonâ€“Raphson** method while delegating derivative computation to **TorchSharpâ€™s automatic differentiation** engine.  The goal is to provide a template for numerical algorithms that can **seamlessly switch between CPU and GPU** execution with only a device flag.

## 2Â Project Structure

| Path                                       | Description                                                    |
| ------------------------------------------ | -------------------------------------------------------------- |
| `RootFinderGPU/NewtonRaphsonRootFinder.cs` | Core static class that performs the Newtonâ€“Raphson iterations. |
| `RootFinderGPU.csproj`                     | .NETÂ 9 project file, references `TorchSharpâ€‘cudaâ€‘windows`.     |
| `RootFinderGPUTest/`                       | MSTest project exercising a wide range of unit tests.          |
| `RootFinderGPU.sln`                        | VisualÂ Studio solution tying everything together.              |

```
RootFinderGPU
â”œâ”€â”€ NewtonRaphsonRootFinder.cs
â””â”€â”€ RootFinderGPU.csproj
RootFinderGPUTest
â”œâ”€â”€ NewtonRaphsonTests.cs
â””â”€â”€ MSTestSettings.cs
RootFinderGPU.sln
```

## 3Â Prerequisites

* **.NETÂ 9Â SDK** (or higher)
* **TorchSharpÂ 0.105.0** â€“ the CUDAâ€‘enabled build is referenced but falls back to CPU if a CUDA device is unavailable.
* A CUDAâ€‘capable GPU **(optional)** â€“ set the device to `torch.CUDA` to unleash GPU acceleration.

## 4Â Building & Running

```bash
# clone your fork
> git clone https://github.com/<user>/RootFinderGPU.git
> cd RootFinderGPU/src

# restore, build, test
> dotnet restore
> dotnet build -c Release
> dotnet test RootFinderGPUTest -c Release
```

All tests should pass; failing tests almost always indicate an issue in the function you supply or in the tolerances chosen.

## 5Â Using the API

### 5.1Â Signature

```csharp
public static double FindRoot(
    Func<torch.Tensor, torch.Tensor> f,
    double initialGuess,
    double tolerance,
    int   maxIterations = 100 )
```

* **`f`**Â â€“ a scalarâ€‘toâ€‘scalar tensor function *f(x)* defined with TorchSharp ops.
* **`initialGuess`**Â â€“ starting point $xâ‚€$.
* **`tolerance`**Â â€“ absolute *|f(x)|* threshold to declare convergence.
* **`maxIterations`**Â â€“ optional guard against infinite loops.

### 5.2Â Example: Finding âˆš2

```csharp
using RootFinderGPU;
using TorchSharp;

// f(x) = xÂ² â€“ 2  â‡’  root = âˆš2 â‰ˆÂ 1.41421356
Func<torch.Tensor, torch.Tensor> f = x => x.pow(2).sub(2.0);

double root = NewtonRaphsonRootFinder.FindRoot(
    f,
    initialGuess: 1.0,
    tolerance: 1e-8);

Console.WriteLine($"âˆš2 â‰ˆ {root}");
```

Switch to GPU by adding:

```csharp
torch.Device device = torch.CUDA;
```

and passing tensors on that device when constructing `x`.  In the current sample this line is commented for portability.

## 6Â AlgorithmÂ Flow

1. **InstantiateÂ x** as a tensor with `requires_grad_()` so TorchSharp tracks operations.
2. **EvaluateÂ f(x)** â€“ may call any differentiable TorchSharp ops.
3. **Backward pass** â€“ `torch.autograd.grad` (or `fx.backward()`) populates `x.grad` with df/dx.
4. **Newton update** â€“ `xâ‚ = xâ‚€ â€“ f(xâ‚€)/fâ€²(xâ‚€)`.
5. **Convergence checks** â€“ tolerance, small derivative, NaN/InfÂ guards, divergence caps.
6. **Iterate or return** â€“ dispose tensors each loop to prevent nativeâ€‘memory leaks.

## 7Â Error Handling & EdgeÂ Cases

| Condition                                     | Return value | Notes                                  |
| --------------------------------------------- | ------------ | -------------------------------------- |
| `f(x)` returnsÂ NaN/Inf                        | `double.NaN` | Function invalid at currentÂ x.         |
| Derivative â‰ˆÂ 0                                | `double.NaN` | Avoids divideâ€‘byâ€‘zero & flat tangents. |
| Divergent step size or guess magnitude >Â 1e10 | `double.NaN` | Likely runaway.                        |
| No convergence after `maxIterations`          | `double.NaN` | Signals caller to adjust guess/params. |

## 8Â Extending the Library

* **Alternate methods** â€“ add Secant, Bisection, or Brent algorithms in sibling classes for robustness where derivatives are unavailable.
* **Vector roots** â€“ wrap TorchSharpâ€™s `torch.autograd.functional.jacobian` to extend Newtonâ€™s method toÂ â„â¿.
* **Batch evaluation** â€“ treat multiple initial guesses in a tensor batch to exploit GPU parallelism.

## 9Â UnitÂ Testing

The `RootFinderGPUTest` project covers:

* Linear, quadratic, cubic, transcendental, and multiâ€‘root polynomials.
* Zeroâ€‘derivative hazards and functions without real roots.
* Convergence failure when iteration cap is deliberately tight.

Use these tests as templates to verify new algorithms.

## 10Â Troubleshooting

| Symptom            | Likely Cause                                             | Fix                                                                |
| ------------------ | -------------------------------------------------------- | ------------------------------------------------------------------ |
| Always returnsÂ NaN | Bad initial guess; derivative zero; tolerance too tight. | Try different guess/tolerance or switch to a bracketâ€‘based method. |
| CUDA outâ€‘ofâ€‘memory | Large batch size or many simultaneous roots.             | Free tensors promptly; reduce batch; monitor GPU memory.           |
| Gradient error     | Function uses nonâ€‘differentiable ops.                    | Replace with differentiable ops or finiteâ€‘difference fallback.     |

## 11Â License &Â Credits

### Happy rootâ€‘finding! ğŸ¯
</br>
Copyright [TranscendAI.tech](https://TranscendAI.tech) 2025.<br>
Authored by Warren Harding. AI assisted.</br>
